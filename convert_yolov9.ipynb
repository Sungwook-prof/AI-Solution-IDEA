{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIPqjJ9ctwEE",
        "outputId": "a640f74a-cb93-43cf-b82e-2ce5097b1b24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/yolov9\n"
          ]
        }
      ],
      "source": [
        "import os, glob\n",
        "from IPython.display import Image\n",
        "from google.colab import drive, userdata\n",
        "\n",
        "\n",
        "HOME = os.getcwd()\n",
        "YOLO = os.path.join(HOME, 'yolov9')\n",
        "print(HOME)\n",
        "print(YOLO)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 구글 드라이브 마운트\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kh-MUbtDt-BP",
        "outputId": "2bb92f6b-6067-459e-bdfa-a18720e53025"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -q \"openvino>=2023.3.0\" \"nncf>=2.8.1\" \"opencv-python\" \"seaborn\" \"pandas\" \"scikit-learn\" \"torch\" \"torchvision\" \"tqdm\"  --extra-index-url https://download.pytorch.org/whl/cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OUlnl_Hwy0V",
        "outputId": "0dddaf3e-9347-4cd6-e446-bbb6a6f51cc0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.4/68.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m102.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.1/249.1 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for jstyleson (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for grapheme (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -q \"matplotlib>=3.4\""
      ],
      "metadata": {
        "id": "9bSTS1sHw5l5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/WongKinYiu/yolov9.git\n",
        "%cd yolov9\n",
        "!pip install -r requirements.txt -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHw7s_Ff0jZZ",
        "outputId": "df716420-d789-4de0-92b2-304b6e6372ee"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov9'...\n",
            "remote: Enumerating objects: 781, done.\u001b[K\n",
            "remote: Total 781 (delta 0), reused 0 (delta 0), pack-reused 781 (from 1)\u001b[K\n",
            "Receiving objects: 100% (781/781), 3.27 MiB | 16.41 MiB/s, done.\n",
            "Resolving deltas: 100% (331/331), done.\n",
            "/content/yolov9\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PyTorch 모델을 OpenVINO IR로 변환\n",
        "\n",
        "OpenVINO는 모델 변환 API를 제공한다. ov.convert_model 함수는 모델 객체와 모델을 분석하기 위한 입력을 받아서, ov.Model 인스턴스를 리턴한다. 리턴된 모델은 특정 장치용으로 로딩하거나 ov.save_model을 사용하여 다음 추론을 위해 저장될 수 있다."
      ],
      "metadata": {
        "id": "_ciT7V11xL0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from models.experimental import attempt_load\n",
        "import torch\n",
        "import openvino as ov\n",
        "from models.yolo import Detect, DualDDetect\n",
        "from utils.general import yaml_save, yaml_load\n",
        "from pathlib import Path\n",
        "\n",
        "MODEL_DIR = Path(\"/content/drive/MyDrive/data/bin/\")\n",
        "weights = MODEL_DIR / \"best.pt\"\n",
        "ov_model_path = MODEL_DIR / weights.name.replace(\".pt\", \"_openvino_model\") / weights.name.replace(\".pt\", \".xml\")\n",
        "\n",
        "if not ov_model_path.exists():\n",
        "    model = attempt_load(weights, device=\"cpu\", inplace=True, fuse=True)\n",
        "    metadata = {\"stride\": int(max(model.stride)), \"names\": model.names}\n",
        "\n",
        "    model.eval()\n",
        "    for k, m in model.named_modules():\n",
        "        if isinstance(m, (Detect, DualDDetect)):\n",
        "            m.inplace = False\n",
        "            m.dynamic = True\n",
        "            m.export = True\n",
        "\n",
        "    example_input = torch.zeros((1, 3, 640, 640))\n",
        "    model(example_input)\n",
        "\n",
        "    ov_model = ov.convert_model(model, example_input=example_input)\n",
        "\n",
        "    # specify input and output names for compatibility with yolov9 repo interface\n",
        "    ov_model.outputs[0].get_tensor().set_names({\"output0\"})\n",
        "    ov_model.inputs[0].get_tensor().set_names({\"images\"})\n",
        "    ov.save_model(ov_model, ov_model_path)\n",
        "    # save metadata\n",
        "    yaml_save(ov_model_path.parent / weights.name.replace(\".pt\", \".yaml\"), metadata)\n",
        "else:\n",
        "    metadata = yaml_load(ov_model_path.parent / weights.name.replace(\".pt\", \".yaml\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kad_pvFPxMJ2",
        "outputId": "a7cdd9a1-76ab-4c49-e271-29889135f713"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/yolov9/models/experimental.py:243: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ckpt = torch.load(attempt_download(w), map_location='cpu')  # load\n",
            "Fusing layers... \n",
            "gelan-c summary: 387 layers, 25233256 parameters, 0 gradients, 101.8 GFLOPs\n",
            "/content/yolov9/models/yolo.py:108: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  elif self.dynamic or self.shape != shape:\n"
          ]
        }
      ]
    }
  ]
}